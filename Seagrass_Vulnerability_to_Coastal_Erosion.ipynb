{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "## Things to add\n",
    "\n",
    "# - [x] Change Gain and Loss has wrong file names they all end with 2021\n",
    "# - [] Check if you can print out the date/month of each image in a year\n",
    "# - [] Automatically plot images either the ee image or image url\n",
    "# - [] Add one satellite image to the map for nicer visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)\n",
    "\n",
    "from contextlib import suppress\n",
    "\n",
    "import ee\n",
    "import logging\n",
    "\n",
    "# fix the warnings/error messages from 'file_cache is unavailable when using oauth2client'\n",
    "# https://github.com/googleapis/google-api-python-client/issues/299\n",
    "logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except ee.ee_exception.EEException:\n",
    "    authenticate()\n",
    "    ee.Initialize()  # retry initialization once the user logs in\n",
    "\n",
    "## Below method doesnt work for Voila\n",
    "\n",
    "# # Trigger the authentication flow & then Initialize the library.\n",
    "# ee.Authenticate()\n",
    "# ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Label\n",
    "\n",
    "\n",
    "import geemap as emap\n",
    "from geemap import geojson_to_ee, ee_to_geojson\n",
    "\n",
    "\n",
    "import ipyleaflet  # an interactive mapping \"widget\"\n",
    "from ipyleaflet import Map, DrawControl, SplitMapControl, FullScreenControl, LayersControl, Marker, MarkerCluster, GeoJSON\n",
    "from IPython.display import Image, display, IFrame, clear_output, HTML, Markdown\n",
    "\n",
    "\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from skimage import io\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import os # file handling\n",
    "import fnmatch # file handling\n",
    "from sklearn.neighbors import BallTree # calculating neares distance between features\n",
    "import geoplot # plotting\n",
    "import geoplot.crs as gcrs # plotting\n",
    "import mapclassify # classifying data\n",
    "from shapely.geometry import Polygon # merging vector files / polygons\n",
    "from shapely.ops import cascaded_union\n",
    "\n",
    "from osgeo import gdal, ogr #osr?\n",
    "import osr\n",
    "\n",
    "import sys\n",
    "\n",
    "#%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm # for colormaps in plots\n",
    "#from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "#import matplotlib.image as mpimg #to read adn display image\n",
    "\n",
    "# To prevent automatic figure display when execution of the cell ends\n",
    "#%config InlineBackend.close_figures=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----------------------------------------##\n",
    "#       Combined Landsat Collection\n",
    "##-----------------------------------------##\n",
    "\n",
    "\n",
    "#def makeLandsatSeries():\n",
    "\n",
    "lt4 = ee.ImageCollection('LANDSAT/LT04/C01/T1_SR')\n",
    "lt5 = ee.ImageCollection('LANDSAT/LT05/C01/T1_SR')\n",
    "le7 = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\n",
    "lc8 = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "\n",
    "lt4 = lt4.select(['B1','B2','B3','B4','B5','B7','pixel_qa'],['blu','grn','red','nir','swir1','swir2','pixel_qa'])\n",
    "lt5 = lt5.select(['B1','B2','B3','B4','B5','B7','pixel_qa'],['blu','grn','red','nir','swir1','swir2','pixel_qa'])\n",
    "le7 = le7.select(['B1','B2','B3','B4','B5','B7','pixel_qa'],['blu','grn','red','nir','swir1','swir2','pixel_qa'])\n",
    "lc8 = lc8.select(['B2','B3','B4','B5','B6','B7','pixel_qa'],['blu','grn','red','nir','swir1','swir2','pixel_qa'])\n",
    "\n",
    "fullCollection = ee.ImageCollection(lt4.merge(lt5).merge(le7).merge(lc8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {\n",
    "        'name':'Landsat All',\n",
    "        'base_collection':(fullCollection\n",
    "        ),\n",
    "        'vis_params':{\n",
    "            'min':0,\n",
    "            'max':0.3,\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'name':'Landsat 4 Collection 1 SR',\n",
    "        'base_collection':(\n",
    "            ee.ImageCollection('LANDSAT/LT04/C01/T1_SR')\n",
    "            .select(['B1','B2','B3','B4','B5','B7','pixel_qa'],['blu','grn','red','nir','swir1','swir2','pixel_qa'])\n",
    "        ),\n",
    "        'vis_params':{\n",
    "            'min':0,\n",
    "            'max':0.3,\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'name':'Landsat 5 Collection 1 SR',\n",
    "        'base_collection':(\n",
    "            ee.ImageCollection('LANDSAT/LT05/C01/T1_SR')\n",
    "            .select(['B1','B2','B3','B4','B5','B7','pixel_qa'],['blu','grn','red','nir','swir1','swir2','pixel_qa'])\n",
    "        ),\n",
    "        'vis_params':{\n",
    "            'min':0,\n",
    "            'max':0.3,\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'name':'Landsat 7 Collection 1 SR',\n",
    "        'base_collection':(\n",
    "            ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\n",
    "            .select(['B1','B2','B3','B4','B5','B7','pixel_qa'],['blu','grn','red','nir','swir1','swir2','pixel_qa'])\n",
    "        ),\n",
    "        'vis_params':{\n",
    "            'min':0,\n",
    "            'max':0.3,\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'name':'Landsat 8 Collection 1 SR',\n",
    "        'base_collection':(\n",
    "            ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "            .select(['B2','B3','B4','B5','B6','B7','pixel_qa'],['blu','grn','red','nir','swir1','swir2','pixel_qa'])\n",
    "        ),\n",
    "        'vis_params':{\n",
    "            'min':0,\n",
    "            'max':0.3,\n",
    "        },\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_dropdown_dict():\n",
    "    return dict([(datasets[i]['name'],i) for i in range(len(datasets))])\n",
    "\n",
    "def get_base_collection():\n",
    "    return datasets[dataset_dropdown.index]['base_collection']\n",
    "\n",
    "def get_vis_params():\n",
    "    return datasets[dataset_dropdown.index]['vis_params']\n",
    "\n",
    "def GetTileLayerUrl(ee_image_object):\n",
    "    map_id = ee.Image(ee_image_object).getMapId()\n",
    "    tile_url_template = \"https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}\"\n",
    "    return tile_url_template.format(**map_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map using GEEMAP\n",
    "\n",
    "Map1 = emap.Map(center=[-13, 48], zoom=8)\n",
    "Map1.add_basemap('CartoDB.DarkMatter')\n",
    "Map1.set_plot_options(add_marker_cluster=False) #plot_type='bar'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DrawControl GEE\n",
    "draw_control = Map1.draw_control\n",
    "\n",
    "# AOI Feature\n",
    "feat_list = []\n",
    "coordinates = []\n",
    "\n",
    "# Years\n",
    "all_years = []\n",
    "df_year = []\n",
    "year_list = []\n",
    "next_year_list = []\n",
    "\n",
    "#Number of Images\n",
    "image_count = [] # Total images per year\n",
    "\n",
    "### IMAGES\n",
    "median_images = [] # store array images\n",
    "median_url = [] # store GEE links\n",
    "mndwi_images = []\n",
    "mndwi_url = [] \n",
    "masked_mndwi_images = []\n",
    "masked_mndwi_url = []\n",
    "canny_gee_images = []\n",
    "canny_gee_url = []\n",
    "\n",
    "bi_median_images = []\n",
    "bi_median_url = []\n",
    "bi_mndwi_images = []\n",
    "bi_mndwi_url = [] \n",
    "bi_masked_images = []\n",
    "bi_masked2_images = []\n",
    "bi_masked_mndwi_url = []\n",
    "bi_canny_gee_images = []\n",
    "bi_canny_gee_url = []\n",
    "\n",
    "\n",
    "#array_image = [] \n",
    "#lap_scipy_image = []\n",
    "#canny_image = []\n",
    "#name_list = []\n",
    "\n",
    "change_images = []\n",
    "change_url = []\n",
    "areas = []\n",
    "areagain = []\n",
    "arealoss = []\n",
    "\n",
    "gain_images = []\n",
    "loss_images = []\n",
    "lossvector = []\n",
    "\n",
    "#file_path = []\n",
    "\n",
    "## TEXTS\n",
    "areaWithYear = []\n",
    "\n",
    "# Handle draw events\n",
    "def handle_draw(self, action, geo_json):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    VISUALISATION\n",
    "    image: The image to visualize.\n",
    "    bands: A list of the bands to visualize.  If empty, the first 3 are used.\n",
    "    gain: The visualization gain(s) to use.\n",
    "    bias: The visualization bias(es) to use.\n",
    "    min: The value(s) to map to RGB8 value 0.\n",
    "    max: The value(s) to map to RGB8 value 255.\n",
    "    gamma: The gamma correction factor(s) to use.\n",
    "    opacity: The opacity scaling factor to use.\n",
    "    palette: The color palette to use. List of CSS color identifiers or hexadecimal color strings (e.g. ['red', '00FF00', 'bluevlolet']).\n",
    "    forceRgbOutput: Whether to produce RGB output even for single-band inputs.\n",
    "    For hough transform available band names: [blu_lines, grn_lines, red_lines, nir_lines, swir1_lines, swir2_lines, pixel_qa_lines]\n",
    "            \n",
    "    MATHEMATICAL OPERATIONS\n",
    "    Link: https://developers.google.com/earth-engine/guides/image_math?hl=en\n",
    "\n",
    "    BOOLEAN OPERATORS\n",
    "    You can check example for relational and boolean operators here:\n",
    "    https://developers.google.com/earth-engine/guides/image_relational?hl=en\n",
    "\n",
    "    The output of relational and boolean operators is either true (1) or false (0). \n",
    "    So the result of bi_mndwi.gt(-0.1) is water true (1) and land false (0).\n",
    "    To mask the 0's (land), you can mask the resultant binary image with itself.\n",
    "\n",
    "    Link: https://developers.google.com/earth-engine/guides/image_relational\n",
    "    \n",
    "    INDICES\n",
    "    There are many remote sensing indices and some are useful to extract warter bodies. \n",
    "    NDVI (vegetation index): negative values are clouds, water and snow. 0.2 to 0.3 shrubs and meadows, 0.6 to 0.8 temperate and tropical forests.\n",
    "    NDWI (water index)\n",
    "    MNDWI (modified water index): water value are high (postive) values (above 0.5). Land is around - 0.5\n",
    "    AWEI (Automated Water Extraction Index): AWEI water values above 0\n",
    "    \n",
    "    MASKING\n",
    "    One can use a single index or combination of multiple indices and then mask it using appropriate values to extract water areas. \n",
    "    However, MNDWI seems to give a more realistic water body image compared to:\n",
    "    Combined NDVI and MNDWI: bi_ndvi.lt(-0.5).And(bi_mndwi.gt(-0.1))\n",
    "    Combined AWEI and MNDWI: bi_awei2.gt(0).And(bi_mndwi.gt(-0.1)) \n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    #### Create directory to save files\n",
    "    folder_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    seagrass_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Seagrass')\n",
    "    if not os.path.exists(seagrass_path):\n",
    "        os.makedirs(seagrass_path)\n",
    "    \n",
    "    out_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output')\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "        \n",
    "    tif_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/TIF')\n",
    "    if not os.path.exists(tif_path):\n",
    "        os.makedirs(tif_path)\n",
    "        \n",
    "    csv_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/CSV')\n",
    "    if not os.path.exists(csv_path):\n",
    "        os.makedirs(csv_path)\n",
    "    \n",
    "    ## Shapefile\n",
    "    #file = file_widget.value\n",
    "    #print(\"file is\", file, \"type is\", type(file))\n",
    "\n",
    "    # Get the 1st object in the file_path list. It's the user input for the local shapefile directory\n",
    "    #path = str(file[0]) # string, '/Users/nat/Desktop/Nat/Thesis/Thesis ArcMap/Channel.shp'\n",
    "    #print(\"path is\", path)\n",
    "            \n",
    "    #if path is 'Enter your shapefile path':\n",
    "        #print(\"You don't have any shapefile.\")\n",
    "    #else:\n",
    "        #with out:\n",
    "            #shapefile = emap.shp_to_ee(path)\n",
    "            #print(\"Type shapefile\", type(shapefile))\n",
    "            #map1.addLayer(shapefile, {}, 'My file', opacity=0.4)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ##--------------------------------##\n",
    "    #  Get User ROI From Draw Control \n",
    "    ##--------------------------------##\n",
    "    global roi_geom\n",
    "    \n",
    "    vis = {'bands': ['swir1', 'grn', 'nir'], 'min': 0, 'max': 3000, 'gamma': [0.95, 1.1, 1]} # some sediement flow is visible, water is mostly black gamma: [0.95, 1.1, 1] OR 'gamma': 1\n",
    "    RGBvis = {'bands': ['red', 'grn', 'blu'], 'min': 0, 'max': 3000, 'gamma': [0.95, 1.1, 1] } # some sediement flow is visible, water is mostly black gamma: [0.95, 1.1, 1] OR 'gamma': 1\n",
    "    mndwi_vis = {'bands': ['grn', 'swir1'], 'min': 0, 'max': 3000, 'gamma': [0.95, 1.1] } #'min': 0.5, 'max': 1, 'gamma': [0.95, 1.1] \n",
    "    palette_vis1 = {'min': -1, 'max': 1, 'palette': ['00FFFF', 'FF0000', '000000']} #CYAN , RED, BLACK\n",
    "    palette_vis2 = {'min': -1, 'max': 1, 'palette': ['FF0000', '000000']} #RED , BLACK\n",
    "    palette_vis3 = {'min': -1, 'max': 1, 'palette': ['99FF00', '000000']} #LIME , BLACK\n",
    "    change_vis = {'min': -1, 'max': 1, 'palette': ['99FF00', '00FFFFFF', 'FF0000']} #LIME , Transparent WHITE, RED\n",
    "    #canny_vis = {'opacity':0.6, 'palette':['43a2ca']}\n",
    "    #hough_vis = {'bands': ['red_lines', 'grn_lines', 'blu_lines']}\n",
    "    \n",
    "    \n",
    "    poly = geo_json['geometry']  #ee.geometry {'type': 'Polygon', 'coordinates': [[[48.488769, -13.522172],....\n",
    "    geom = geojson_to_ee(geo_json, False)\n",
    "    geom_info = geom.getInfo()\n",
    "    feature = ee.Feature(geom)\n",
    "    info = feature.getInfo()\n",
    "    feat_list.append(feature)\n",
    "    collection = ee.FeatureCollection(feat_list)\n",
    "    \n",
    "    # Get user ROI\n",
    "    roi = ee.FeatureCollection(Map1.draw_last_feature) # OPTIONS:Map.draw_last_feature, Map.draw_features\n",
    "    roi_geom = roi.geometry() # ee.geometry\n",
    "    \n",
    "    #Compute the centroid of the polygon.\n",
    "    #centroid = roi_geom.centroid() # ee.geometry\n",
    "    centroid = roi_geom.centroid(100) # ee.geometry 100= max error\n",
    "    \n",
    "    #Get the centroid point coordinates and save them\n",
    "    coord = centroid.getInfo()['coordinates']\n",
    "    coordinates.append(coord)\n",
    "    \n",
    "    ##### Export ROI Feauture Collection\n",
    "    # Select Path\n",
    "    roi_out = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/ROI')\n",
    "    \n",
    "    # Create Folder\n",
    "    if not os.path.exists(roi_out):\n",
    "        os.makedirs(roi_out)\n",
    "    \n",
    "    # Download\n",
    "    roi_file = os.path.join(roi_out, 'ROI.shp')\n",
    "    emap.ee_to_shp(roi, filename=roi_file)\n",
    "\n",
    "    \n",
    "##--------------------------------##\n",
    "#  Get User Inputs\n",
    "##--------------------------------##\n",
    "    \n",
    "    # Get Collection\n",
    "    base_collection = get_base_collection() \n",
    "    \n",
    "    # Get Date & Cloud & Land Mask\n",
    "    start_year = year_picker.value[0]\n",
    "    end_year = year_picker.value[1]\n",
    "    #print(\"start_year\", start_year, \"end_year\", end_year )\n",
    "    #print(\"year_picker.value[0]\",year_picker.value[0])\n",
    "    cloud = cloud_slider.value\n",
    "    #land = land_mask.value\n",
    "    \n",
    "    #Get total years from user input (end - start)\n",
    "    total_years = end_year - start_year\n",
    "    #print(\"total_years\", total_years)\n",
    "    \n",
    "    # Google Earth Engine Start & End Years (From user input)\n",
    "    collection = (\n",
    "        base_collection\n",
    "            .filterDate('{}-01-01'.format(year_picker.value[0]), '{}-01-01'.format(year_picker.value[1]))\n",
    "            .filterBounds(roi)\n",
    "            .filter(ee.Filter.lt('CLOUD_COVER',cloud)))\n",
    "\n",
    "    \n",
    "    #  Create Annual Collections by using date range (ex. 2017-2018, 2018-2019, ...) & Get Annual Median\n",
    "    for i in range(total_years+1):\n",
    "        \n",
    "        ## ----------------------------- ##\n",
    "        #  Date\n",
    "        ## ----------------------------- ##\n",
    "\n",
    "        year = start_year+(i)\n",
    "        all_years.append(year)\n",
    "        following_year = year + 1\n",
    "        #print(\"Year range\", year, following_year )\n",
    "        # Append the year if the amount of images is > 10. Otherwise there is no need for year data. \n",
    "        # This is is done below // year_list.append(year)\n",
    "        \n",
    "        \n",
    "        ## ------------------------------- ##\n",
    "        #  Filter Collection For Each Year\n",
    "        ## ------------------------------- ##\n",
    "\n",
    "        interval_collection = (\n",
    "            base_collection\n",
    "                .filterDate('{}-01-01'.format(year), '{}-01-01'.format(following_year))\n",
    "                .filterBounds(roi)\n",
    "                .filter(ee.Filter.lt('CLOUD_COVER',cloud)))\n",
    "      \n",
    "       \n",
    "        \n",
    "        # Resample to bicubic (GEE originally performs nearest neighbourhood resampling which is not smooth)\n",
    "        bicubic_collection = interval_collection.map(lambda i: i.resample('bicubic')) #median.resample('bicubic') --Doesnt work\n",
    "        \n",
    "        # Get amount of imagery\n",
    "        count_filtered = bicubic_collection.size().getInfo()\n",
    "        image_count.append(count_filtered)\n",
    "        #print(\"Number of images between\", \"{}-01-01\".format(year), \"{}-01-01\".format(following_year), \"is\", count_filtered)\n",
    "        \n",
    "        \n",
    "        ## ------------------------------- ##\n",
    "        #  Annual Median Image\n",
    "        ## ------------------------------- ##\n",
    "        \n",
    "        # Create Annual Median Image\n",
    "        # If the number of images for a year is greater then 10,\n",
    "        # calculate the annual median, mndwi, mask & canny edge, add to lists, map them, get URL. Else\n",
    "        # the number of images is insufficient to create an annaul median image. Because,\n",
    "        # we need sufficient number of overlapping pixels to eliminate seasonal changes, clouds and shadows\n",
    "        #if count_filtered > 10:\n",
    "        if count_filtered > 6:\n",
    "            \n",
    "            print(\"Number of images between\", \"{}\".format(year), \"{}\".format(following_year), \"is\", count_filtered)\n",
    "            \n",
    "            year_list.append(year)        \n",
    "        \n",
    "            ## ------------------------------------------------------------- ##\n",
    "            # Neighrest Neigh. Resampling: Median & MNDWI & Mask & Canny\n",
    "            ## ------------------------------------------------------------- ##\n",
    "            median = interval_collection.median().clip(roi) #Median reducer method and clip to ROI\n",
    "            median_images.append(median) #Append to list\n",
    "            #map1.addLayer(median, RGBvis, 'RGB Median {}'.format(year)) #Add to map\n",
    "            \n",
    "            median_thumb = median.visualize(**RGBvis).getThumbURL({'region' : roi_geom, 'format': 'png', 'dimensions': 700}) #'dimensions': 1024 // Get thumb URL\n",
    "            median_url.append(median_thumb) #url links. type string //Append to list\n",
    "            \n",
    "            mndwi = median.normalizedDifference(['grn', 'swir1']).rename('MNDWI') #Calculate mndwi \n",
    "            mndwi_images.append(mndwi) #Append to list\n",
    "            #Map1.addLayer(mndwi, palette_vis2, 'MNDWI {}'.format(year))  \n",
    "            \n",
    "            mndwi_thumb = mndwi.visualize(**palette_vis2).getThumbURL({'region' : roi_geom, 'format': 'png', 'dimensions': 700})\n",
    "            mndwi_url.append(mndwi_thumb)\n",
    "            \n",
    "            water = mndwi.gt(-0.1) # one band image. Type 'ee.image.Image'> \n",
    "            masked_mndwi_images.append(water)\n",
    "            \n",
    "            masked_mndwi_thumb = water.visualize(**palette_vis2).getThumbURL({'region' : roi_geom, 'format': 'png', 'dimensions': 700})\n",
    "            masked_mndwi_url.append(masked_mndwi_thumb)\n",
    "            \n",
    "            #canny = ee.Algorithms.CannyEdgeDetector(water, 0.7)\n",
    "            #canny_gee_images.append(canny)\n",
    "            #Map1.addLayer(canny.updateMask(canny), {'palette': 'FF0000'}, 'Neighrest  Canny {}'.format(year))\n",
    "            \n",
    "            #canny_thumb = canny.visualize(**palette_vis2).getThumbURL({'region' : roi_geom, 'format': 'png', 'dimensions': 700})\n",
    "            #canny_gee_url.append(canny_thumb)\n",
    "\n",
    "            ## ------------------------------------------------------------- ##\n",
    "            #  Bicubic Resampling: Median & MNDWI & Mask  \n",
    "            ## ------------------------------------------------------------- ##\n",
    "            \n",
    "            # Get median reducer image & clip from the bicubic resampled collection\n",
    "            bi_median = bicubic_collection.median().clip(roi)\n",
    "            bi_median_images.append(bi_median)\n",
    "            #map1.addLayer(bi_median, RGBvis, 'RGB Median {}'.format(year))\n",
    "            \n",
    "            \n",
    "            # Get URL for the median image\n",
    "            bi_median_thumb = bi_median.visualize(**RGBvis).getThumbURL({'region' : roi_geom, 'format': 'png', 'dimensions': 700}) #'dimensions': 1024\n",
    "            bi_median_url.append(bi_median_thumb)\n",
    "            \n",
    "            # Calculate Indices & Get URL\n",
    "            # AWEI: B1 blu, B2 grn, B4 nir, B5 swir1, B7 swir2\n",
    "            #bi_ndvi = bi_median.normalizedDifference(['nir', 'red']).rename('NDVI')\n",
    "            bi_mndwi = bi_median.normalizedDifference(['grn', 'swir1']).rename('MNDWI') #Calculate mndwi \n",
    "            bi_mndwi_images.append(mndwi) #Append to list\n",
    "            bi_mndwi_thumb = bi_mndwi.visualize(**palette_vis2).getThumbURL({'region' : roi_geom, 'format': 'png', 'dimensions': 700})\n",
    "            bi_mndwi_url.append(bi_mndwi_thumb)\n",
    "\n",
    "            \n",
    "            ##-------------------##\n",
    "            #  Water & Save TIF\n",
    "            ##-------------------##\n",
    "            \n",
    "            # Mask land from MNDWI & get \n",
    "            bi_water = bi_mndwi.gt(-0.1) # one band image. Type 'ee.image.Image'>\n",
    "            bi_masked_images.append(bi_water)\n",
    "            bi_masked_mndwi_thumb = bi_water.visualize(**palette_vis2).getThumbURL({'region' : roi_geom, 'format': 'png', 'dimensions': 700})\n",
    "            bi_masked_mndwi_url.append(bi_masked_mndwi_thumb)\n",
    "            #Map1.addLayer(bi_water, palette_vis2, 'MNDWI Mask {}'.format(year))\n",
    "                 \n",
    "            \n",
    "            # Save Water (Land Mask) & MNDWI\n",
    "            #out_dir = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index')\n",
    "            \n",
    "            \n",
    "            #out_file = os.path.join(out_dir, 'MNDWI{}-{}.tif'.format(year, following_year))\n",
    "            #emap.ee_export_image(bi_mndwi, filename=out_file, scale=12, region=roi_geom) \n",
    "            \n",
    "            #out_file1 = os.path.join(out_dir, 'Water{}-{}.tif'.format(year, following_year)) #following_year\n",
    "            #emap.ee_export_image(bi_water, filename=out_file1, scale=10, region=roi_geom) # scale=90\n",
    "\n",
    "            ##-----------------##\n",
    "            #  Edge & Save TIF\n",
    "            ##-----------------##  \n",
    "            \n",
    "            # Determine Edges Using Canny Edge Detector & Get URL\n",
    "            #bi_canny = ee.Algorithms.CannyEdgeDetector(bi_water, 0.7)\n",
    "            #bi_canny_gee_images.append(bi_canny)\n",
    "            \n",
    "            # Visualising Canny\n",
    "            #Map1.addLayer(bi_canny.updateMask(bi_canny), {'palette': 'FF0000'}, 'Canny {}'.format(year))\n",
    "            #bi_canny_thumb = bi_canny.visualize(**palette_vis2).getThumbURL({'region' : roi_geom, 'format': 'png', 'dimensions': 700})\n",
    "            #bi_canny_gee_url.append(bi_canny_thumb)\n",
    "            \n",
    "            # Save \n",
    "            #out_file2 = os.path.join(out_dir, 'Canny{}-{}.tif'.format(year, following_year))\n",
    "            #emap.ee_export_image(bi_canny, filename=out_file2, scale=12, region=roi_geom) # scale=90\n",
    "            \n",
    "           \n",
    "            \n",
    "            ## ------------------------------- ##\n",
    "            #  Land Mask Info\n",
    "            ## ------------------------------- ##\n",
    "            '''\n",
    "            How to Mask?\n",
    "            In MNDWI water has high positive values, land has negative values\n",
    "            In NDVI water: values close to -1 & Rock, sand, snow: 0.1 and -0.1 & Shrub, grassland: 0.2 to 0.3 & temperate and tropical rainforests: 0.6 to 0.8\n",
    "            \n",
    "            You can create a binary layer using logical operations to mask land. \"lt\" is lessthan, \"gt\" is greaten then.\n",
    "            If needed you can combine ndwi & mndwi values to mask:\n",
    "            water = ndvi.lt(-0.8).And(mndwi.gt(-0.1))\n",
    "            If you only want use MNDWI to mask then:\n",
    "            water = mndwi.gt(-0.1)\n",
    "            \n",
    "            -----\n",
    "            \n",
    "            Choosing a value to mask:\n",
    "            In MNDWI the water has high positive values and land has low negative values. But mangrove and turbid water areas have a bit mixed values. \n",
    "            In such areas it is possible to observe  -0.2 and -0.1 respectively. To prevent turbid water from being masked out as land, \n",
    "            you can select the areas with a value greater than -0.1\n",
    "            \n",
    "            water = mndwi.gt(-0.1)\n",
    "            \n",
    "            -----\n",
    "            \n",
    "            Getting Info About Resulting Image:\n",
    "            water_band = water.bandNames() #Get Band Info \n",
    "            water_prop = water.propertyNames() # Get properties\n",
    "            print(\"water band\", water_band)\n",
    "            print(\"water property\", water_prop )\n",
    "            \n",
    "            Get info 200meters (half km) of user ROI\n",
    "            info_water = water.getRegion(roi,200).getInfo()\n",
    "            header_water = info_median[0] \n",
    "            \n",
    "            -----\n",
    "            \n",
    "            Allowing User to Chose a Land Mask Value:\n",
    "            You can allow users to chose mask value (land). A widget is already created\n",
    "\n",
    "            mndwi_mask_value = float(\"{0:.1f}\".format(land)) # \".1f\" is to represent number with only one digit\n",
    "            ndvi_mask_value = float(\"{0:.1f}\".format(land)) ## IF you want the user to be able to change the NDVI mask option create another float slider widget\n",
    "            mndwi_Masked = mndwi.updateMask(mndwi.gte(mndwi_mask_value).And(ndvi.lt(-0.1)))\n",
    "            \n",
    "            '''\n",
    "             \n",
    "            \n",
    "        else:\n",
    "            print(\"Number of images between\", \"{}\".format(year), \"{}\".format(following_year), \"is is less than 6. Not enough for median reducer.\")\n",
    "            #print(\"The number of imagery between {} and {} is less than 6\".format(year, following_year))\n",
    "    \n",
    "\n",
    "    ##------------------------------------##\n",
    "    #  Raster Calulation\n",
    "    ##------------------------------------##\n",
    "    \n",
    "    '''\n",
    "    NUMBER OF PIXELS:\n",
    "    Calculate the number of pixels composing objects using the connectedPixelCount()\n",
    "    connectedPixelCount() returns a copy of the input image \n",
    "    Link: https://developers.google.com/earth-engine/guides/image_objects\n",
    "    \n",
    "    PIXEL AREA:\n",
    "    Calculate object area by multiplying the area of a single pixel by the number of pixels (determined by connectedPixelCount()). \n",
    "    Pixel area is provided by an image generated from ee.Image.pixelArea().\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for i in range(0, len(bi_masked_images)-1): # 2017 2018 2019 -- 3 images 2 change 17to18 and 18to19 \n",
    "        \n",
    "        '''\n",
    "        PROJECTION\n",
    "        An image which is a composite or mosaic of input images with different projections will have the default projection, \n",
    "        which is WGS84 with 1-degree scale.\n",
    "        '''\n",
    "    \n",
    "        ##---------##\n",
    "        #  Years \n",
    "        ##---------##      \n",
    "        # We are using the years saved in a list because not all user input years might be included in the calculation\n",
    "        # Eg. Between year 2005 and 2020, the year 2007 might have images less than 10. In this case we are skipping this year\n",
    "        year = year_list[i]\n",
    "        df_year.append(year)\n",
    "        next_year = year + 1\n",
    "        #print(\"Year\", year, \"Next year\", next_year )\n",
    "        str_year = str(year_list[i])\n",
    "        str_next_year = str(year_list[i+1])\n",
    "       \n",
    "        ##--------------------------##\n",
    "        #  Legend (For Change Image)\n",
    "        ##--------------------------##       \n",
    "        \n",
    "        #legend_keys = ['Loss', 'No Change', 'Gain']\n",
    "        #legend_colors = ['#1458c7', '#fff8d8', '#a4c100' ] # '#FB8072' '#FFFFB3'\n",
    "        #Map1.add_legend(legend_keys=legend_keys, legend_colors=legend_colors, position='bottomright')\n",
    "        \n",
    "        ##---------------------------------------------------##\n",
    "        #  Subtract Next Year From This Year (Ex. 2019 - 2020)\n",
    "        ##---------------------------------------------------##\n",
    "        '''\n",
    "        It doesnt matter what you subtract from what. 2019-2020 is same as 2020-2019.\n",
    "        Only the loss and gain numbers will change. \n",
    "        \n",
    "        2019   -   2020 =  Result\n",
    "        1 Water  1 Water  0 No Change\n",
    "        1 Water  0 Land   1 Sedimentation, Gain, or Sea Level Decrease\n",
    "        0 Land   0 Land   0 No Change\n",
    "        0 Land   1 Water -1 Erosion, Loss, Sea Level Increase\n",
    "        \n",
    "        '''\n",
    "        difference = bi_masked_images[i].subtract(bi_masked_images[i+1]) # Type: <class 'ee.image.Image'>, With Band MNDWI Example: {'type': 'Image', 'bands': [{'id': 'MNDWI', 'data_type': {'type': 'PixelType', 'precision': 'int', 'min': -1, 'max': 1}, 'crs': 'EPSG:4326', 'crs_transform': [1, 0, 0, 0, 1, 0]}]}\n",
    "        #change_images.append(difference)\n",
    "        #Map1.addLayer(difference.updateMask(difference),{'bands': ['MNDWI'], 'min': -1, 'max': 1, 'palette': legend_colors},'Change {}-{}'.format(str_year, str_next_year), True)  \n",
    "        change_thumb = difference.visualize(**palette_vis2).getThumbURL({'region' : roi_geom, 'format': 'png', 'dimensions': 700})\n",
    "        change_url.append(change_thumb)\n",
    "       \n",
    "        \n",
    "        ##-----------------------##\n",
    "        #  Mask & Create New Band  \n",
    "        ##-----------------------##        \n",
    "\n",
    "        \n",
    "        gain = difference.gt(0).rename('Gain')\n",
    "        gain_images.append(gain)\n",
    "        loss = difference.lt(0).rename('Loss') #.selfMask()   \n",
    "        loss_images.append(loss)\n",
    "        \n",
    "        #loss_vis = loss.updateMask(loss).visualize(**{'bands': ['Loss'],'min': 0,'max': 1,'palette': ['48cae4']}) #(loss.updateMask(loss),{'bands': ['Loss']},'Loss {}-{}'.format(str_year, str_next_year), True) # Type is ee.Image\n",
    "        #loss_images.append(loss_vis)\n",
    "        \n",
    "        #Map1.addLayer(loss.updateMask(loss),{'bands': ['Loss']},'Loss {}-{}'.format(str_year, str_next_year), True)\n",
    "        Map1.addLayer(loss.updateMask(loss),{'bands': ['Loss'], 'min': -1, 'max': 1, 'palette': ['48cae4']},'Loss {}-{}'.format(str_year, str_next_year), True)\n",
    "        difference = difference.addBands([gain, loss])\n",
    "        #change_images.append(difference)\n",
    "        \n",
    "        \n",
    "        ##---------##\n",
    "        #  Area / Sum \n",
    "        ##---------##        \n",
    "        # ee.Image.pixelArea is pixel area image. Then using the bands created above print out the sum of pixel area\n",
    "        # Multiply pixel area by the number of pixels in an object to calculate\n",
    "        area_lossgain = difference.select(['Gain', 'Loss']).multiply(ee.Image.pixelArea()).reduceRegion(reducer=ee.Reducer.sum(), geometry=roi_geom, scale=15) \n",
    "        \n",
    "        ## Loss and Gain metre square\n",
    "        area_lossgain_info = area_lossgain.getInfo() # Areas {'Erosion': 92215.77574268416, 'Sedimentation': 15716.88549041748}\n",
    "        if area_lossgain_info is not None:\n",
    "            areas.append(str(area_lossgain_info))\n",
    "            area_with_year = \"Area Information {}-{}\".format(str_year, str_next_year), area_lossgain_info\n",
    "            areaWithYear.append(area_with_year)\n",
    "            print(\"Area Information {}-{}\".format(str_year, str_next_year), area_lossgain_info)\n",
    "        else:\n",
    "            print(\"No Area Info Available\")\n",
    "        \n",
    "        area_loss = difference.select(['Loss']).multiply(ee.Image.pixelArea()).reduceRegion(reducer=ee.Reducer.sum(), geometry=roi_geom, scale=15) #, crs='EPSG:4979'\n",
    "        area_loss_info = area_loss.getInfo()\n",
    "        arealoss.append(area_loss_info)\n",
    "        area_gain = difference.select(['Gain']).multiply(ee.Image.pixelArea()).reduceRegion(reducer=ee.Reducer.sum(), geometry=roi_geom, scale=15) #, crs='EPSG:4979'\n",
    "        area_gain_info = area_gain.getInfo()\n",
    "        areagain.append(area_gain_info)\n",
    "        \n",
    "        # Make a 3d array from year, loss and gain\n",
    "        #areas_3array[0].append(year)\n",
    "        #areas_3array[1].append(area_loss_info)\n",
    "        #areas_3array[2].append(area_gain_info)\n",
    "        #print(areas_3array)\n",
    "        \n",
    "            \n",
    "        #area_KmSq = ee.Number(area.get(difference.bandNames().getInfo()[0])).divide(1000*1000)\n",
    "        #areaKmSq[0].append(year)\n",
    "        #areaKmSq[1].append(area_KmSq)\n",
    "        #print(\"Array KmSq\", areaKmSq) #Array KmSq [[2018, 2019], [<ee.ee_number.Number object at 0x7f7ffdfb3410>, <ee.ee_number.Number object at 0x7f7ffdfbfd90>]]\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        ##---------##\n",
    "        #  Save\n",
    "        ##---------##     \n",
    "        \n",
    "        #out_file3 = os.path.join(out_dir, 'Loss{}-{}.tif'.format(year, next_year))\n",
    "        #emap.ee_export_image(loss, filename=out_file3, scale=10, region=roi_geom) \n",
    "        \n",
    "        #out_file4 = os.path.join(out_dir, 'Gain{}-{}.tif'.format(year, next_year))\n",
    "        #emap.ee_export_image(gain, filename=out_file4, scale=10, region=roi_geom) #tileScale=2 no such argument in ee_export_image\n",
    "        \n",
    "        #out_file5 = os.path.join(out_dir, 'Change{}-{}.tif'.format(year, next_year))\n",
    "        #emap.ee_export_image(difference, filename=out_file5, scale=12, region=roi_geom) #tileScale=2 no such argument in ee_export_image\n",
    "    \n",
    "    # Save the last Change image only\n",
    "    y1 = year_list[-2]\n",
    "    y2 = year_list[-1]\n",
    "    \n",
    "    #image = change_images[-1]\n",
    "    #out_file5 = os.path.join(out_path, 'Change{}-{}.tif'.format(y1, y2))\n",
    "    #emap.ee_export_image(image, filename=out_file5, scale=15, region=roi_geom) #tileScale=2 no such argument in ee_export_image\n",
    "    \n",
    "    # Save the last MNDWI image\n",
    "    mndwi_im = bi_mndwi_images[-1]\n",
    "    #Map1.addLayer(mndwi_im,{'bands': ['MNDWI'], 'min': -1, 'max': 1, 'palette': legend_colors},'MNDWI {}-{}'.format(str_year, str_next_year), True)\n",
    "    out_file6 = os.path.join(tif_path, 'MNDWI{}-{}.tif'.format(y1, y2))\n",
    "    emap.ee_export_image(mndwi_im, filename=out_file6, scale=15, region=roi_geom)\n",
    "    \n",
    "    ##-----------------------------------------##\n",
    "    #  Create Image Collection & Reduce with Max\n",
    "    ##-----------------------------------------##   \n",
    "    '''\n",
    "    Creating image collection from individual images:\n",
    "    https://developers.google.com/earth-engine/guides/ic_creating\n",
    "    \n",
    "    Image Reduction:\n",
    "    https://developers.google.com/earth-engine/guides/reducers_image_collection\n",
    "    '''\n",
    "    ####### Reduce & Save \n",
    "\n",
    "    loss_collection = ee.ImageCollection([loss_images]) #image collection\n",
    "    loss_max = ee.Image(loss_collection.max()).selfMask() #ee.Image(loss_collection.max())\n",
    "    loss_out = os.path.join(tif_path, 'Total_Max_Loss.tif')\n",
    "    emap.ee_export_image(loss_max.updateMask(loss_max), filename=loss_out, scale=15, region=roi_geom)\n",
    "    \n",
    "    gain_collection = ee.ImageCollection([gain_images]) #image collection\n",
    "    gain_max = ee.Image(gain_collection.max()).selfMask() #ee.Image(loss_collection.max())\n",
    "    gain_out = os.path.join(tif_path, 'Total_Max_Gain.tif')\n",
    "    emap.ee_export_image(gain_max.updateMask(gain_max), filename=gain_out, scale=15, region=roi_geom)\n",
    "    \n",
    "  \n",
    "    \n",
    "    return bi_masked_mndwi_url\n",
    "\n",
    "\n",
    "draw_control.on_draw(handle_draw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Satellite Widgets\n",
    "\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=get_dataset_dropdown_dict(),\n",
    "    description='Dataset:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "year_picker = widgets.IntRangeSlider(\n",
    "    value=[2000, 2020],\n",
    "    min=1980,\n",
    "    max=2020,\n",
    "    step=1,\n",
    "    description='Year:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    ")\n",
    "\n",
    "cloud_slider = widgets.IntSlider(\n",
    "    value=30,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Cloud cover',\n",
    "    style={'description_width': 'initial'},\n",
    "    #layout=widgets.Layout(width='200px'),\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Display Imagery',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to Display Imagery',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "button_riskindex = widgets.Button(\n",
    "    description='Cumulative Seagrass Risk Index',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Create cumulative index files',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "button_riskindex_year = widgets.Button(\n",
    "    description='Yearly Seagrass Risk Index',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Create yearly index files',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "button_csv = widgets.Button(\n",
    "    description='Save loss & gain csv file',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Save loss and gain area info',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "button_csv2 = widgets.Button(\n",
    "    description='csv test',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Save loss and gain area info',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "button_imagecount = widgets.Button(\n",
    "    description='Total Images Per Year',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Total Images Per Year',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# file_widget = widgets.Text(\n",
    "#     value='Enter your shapefile path',\n",
    "#     placeholder='Type something',\n",
    "#     description='Shapefile:',\n",
    "#     disabled=False\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an output widget (mainly for debugging messages)\n",
    "out = widgets.Output()\n",
    "out.clear_output()\n",
    "#out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18875354173f4efcb4b1c46398a114c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Map(center=[-13, 48], controls=(WidgetControl(options=['position'], widget=HBox(children=(Toggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/tables/08b2187ec5b4cb889506b16822897a0c-2481a88d38603ff8683bdd0705388894:getFeatures\n",
      "Please wait ...\n",
      "Data downloaded to /Users/nat/Downloads/Seagrass_Risk_Index/ROI/ROI.shp\n",
      "Number of images between 1990 1991 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 1991 1992 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 1992 1993 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 1993 1994 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 1994 1995 is 9\n",
      "Number of images between 1995 1996 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 1996 1997 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 1997 1998 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 1998 1999 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 1999 2000 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 2000 2001 is 13\n",
      "Number of images between 2001 2002 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 2002 2003 is is less than 6. Not enough for median reducer.\n",
      "Number of images between 2003 2004 is 8\n",
      "Number of images between 2004 2005 is 12\n",
      "Number of images between 2005 2006 is 11\n",
      "Number of images between 2006 2007 is 15\n",
      "Number of images between 2007 2008 is 9\n",
      "Number of images between 2008 2009 is 11\n",
      "Number of images between 2009 2010 is 11\n",
      "Number of images between 2010 2011 is 10\n",
      "Number of images between 2011 2012 is 10\n",
      "Number of images between 2012 2013 is 8\n",
      "Number of images between 2013 2014 is 28\n",
      "Number of images between 2014 2015 is 23\n",
      "Number of images between 2015 2016 is 26\n",
      "Number of images between 2016 2017 is 28\n",
      "Number of images between 2017 2018 is 31\n",
      "Number of images between 2018 2019 is 26\n",
      "Number of images between 2019 2020 is 22\n",
      "Number of images between 2020 2021 is 15\n",
      "Area Information 1994-2000 {'Gain': 5321496.046221326, 'Loss': 267011.93099975586}\n",
      "Area Information 2000-2003 {'Gain': 1065666.479015515, 'Loss': 3695349.3828783226}\n",
      "Area Information 2003-2004 {'Gain': 2106782.519453939, 'Loss': 1414234.261995622}\n",
      "Area Information 2004-2005 {'Gain': 2261106.382401111, 'Loss': 2994429.0766931265}\n",
      "Area Information 2005-2006 {'Gain': 3683002.2691305126, 'Loss': 598663.1658299465}\n",
      "Area Information 2006-2007 {'Gain': 849604.836402923, 'Loss': 1408181.8424115349}\n",
      "Area Information 2007-2008 {'Gain': 2043701.8574879365, 'Loss': 721018.9172276516}\n",
      "Area Information 2008-2009 {'Gain': 144778.0839655259, 'Loss': 4784071.197512518}\n",
      "Area Information 2009-2010 {'Gain': 1331102.467701392, 'Loss': 1105000.6468313779}\n",
      "Area Information 2010-2011 {'Gain': 732703.0157117657, 'Loss': 1295173.3103748397}\n",
      "Area Information 2011-2012 {'Gain': 2294527.6828773646, 'Loss': 1077104.4286539115}\n",
      "Area Information 2012-2013 {'Gain': 2329889.8827762376, 'Loss': 526857.7821350098}\n",
      "Area Information 2013-2014 {'Gain': 824830.9172640332, 'Loss': 883911.6517070098}\n",
      "Area Information 2014-2015 {'Gain': 282196.3800348618, 'Loss': 1248772.8211978688}\n",
      "Area Information 2015-2016 {'Gain': 811490.4818590949, 'Loss': 244998.698179058}\n",
      "Area Information 2016-2017 {'Gain': 1208594.8211937998, 'Loss': 389388.58251821477}\n",
      "Area Information 2017-2018 {'Gain': 621792.7654374664, 'Loss': 672219.1839212455}\n",
      "Area Information 2018-2019 {'Gain': 343025.26849544747, 'Loss': 1299854.20048505}\n",
      "Area Information 2019-2020 {'Gain': 380393.6396195954, 'Loss': 1286346.3155066397}\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/a08025f31cb59b4df3bef4396d1b10b9-80d4de24f1bcce6a6ae74ed5e282ee59:getPixels\n",
      "Please wait ...\n",
      "Data downloaded to /Users/nat/Downloads/Seagrass_Risk_Index/Output/TIF/MNDWI2019-2020.tif\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/031bfb8e24ab5a00da0b85fc316b1ad3-bf35dfd7eb45d52e832a7c9781290bfe:getPixels\n",
      "Please wait ...\n",
      "Data downloaded to /Users/nat/Downloads/Seagrass_Risk_Index/Output/TIF/Total_Max_Loss.tif\n",
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/c58bfe27d179837a526189f5e12ef182-6c466860f6e6887fbebf341951a58210:getPixels\n",
      "Please wait ...\n",
      "Data downloaded to /Users/nat/Downloads/Seagrass_Risk_Index/Output/TIF/Total_Max_Gain.tif\n"
     ]
    }
   ],
   "source": [
    "# Layout the UI elements.\n",
    "panel = widgets.Box(\n",
    "    [\n",
    "        Map1,\n",
    "        widgets.VBox([\n",
    "            dataset_dropdown,\n",
    "            year_picker,\n",
    "            cloud_slider\n",
    "        ])\n",
    "        #widgets.VBox(children=(out,button))\n",
    "    ],\n",
    "    layout=widgets.Layout(\n",
    "        display='flex',\n",
    "        flex_flow='row',\n",
    "        flex_wrap='wrap'\n",
    "        #height='700px' #50%\n",
    "    )\n",
    ")\n",
    "display(panel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot & Export CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d000ac75634f410eb8895faa7797c72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Button(description='Save loss & gain csv file', icon='check', layout=Layout(width='30…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'areas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-23ffcb8c96a6>\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m######################## 1: Create dataframe from area list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0marea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Result'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mareas\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0marea\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Result'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marea\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0marea\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Result'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marea\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'areas' is not defined"
     ]
    }
   ],
   "source": [
    "out2=widgets.Output()\n",
    "#out.clear_output()\n",
    "vbox = widgets.VBox(children=(out2,button_csv))\n",
    "display(vbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Button\n",
    "def csv(b):\n",
    "   \n",
    "    ######################## 1: Create dataframe from area list \n",
    "    area = pd.DataFrame({'Result':areas})\n",
    "    area['Result'] = area['Result'].str.replace('{','')\n",
    "    area['Result'] = area['Result'].str.replace('}','')\n",
    "    area['Result'] = area['Result'].str.replace(\"'\",'')\n",
    "    area[['Gain','Loss']] = area.Result.str.split(\",\",expand=True) \n",
    "    area['Gain'] = area['Gain'].str.replace(\"Gain:\",'')\n",
    "    area['Loss'] = area['Loss'].str.replace(\"Loss:\",'')\n",
    "\n",
    "    df = pd.DataFrame({'Year':df_year})\n",
    "    #print (df.dtypes)\n",
    "\n",
    "    ######################## 2: Edit dataframe\n",
    "    # making separate first name column from new data frame \n",
    "    df[\"Loss\"]= area['Loss']\n",
    "    df[\"Gain\"]= area['Gain']\n",
    "\n",
    "    # Convert String to Float\n",
    "    df['Loss'] = df['Loss'].astype(float) #.round(decimals=2)\n",
    "    df['Gain'] = df['Gain'].astype(float)\n",
    "    df['Net'] = df['Loss'] - df['Gain'] #df['Gain'] - df['Loss'] ? \n",
    "    #print (df.dtypes)\n",
    "\n",
    "    df['Year'] = pd.to_datetime(df['Year'], format='%y', errors='ignore')\n",
    "\n",
    "    ######################## 3: Save\n",
    "\n",
    "    # Save to CSV\n",
    "    out_dir = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/CSV')\n",
    "    df_outfile = os.path.join(out_dir, 'Loss_Gain_Net.csv')\n",
    "    df.to_csv(df_outfile, index = True)\n",
    "    \n",
    "    # Open CSV\n",
    "    df2 = pd.read_csv(df_outfile)\n",
    "    df2.set_index('Year')\n",
    "    # print(csv_df)\n",
    "\n",
    "    fig1 = go.Figure()\n",
    "\n",
    "    # Add traces\n",
    "    fig1.add_trace(go.Scatter(x=df2['Year'], y=df2['Gain'],\n",
    "                        mode='lines+markers',\n",
    "                        name='Gain'))\n",
    "    fig1.add_trace(go.Scatter(x=df2['Year'], y=df2['Loss'],\n",
    "                        mode='lines+markers',\n",
    "                        name='Loss'))\n",
    "    fig1.add_trace(go.Scatter(x=df2['Year'], y=df2['Net'],\n",
    "                        mode='lines+markers',\n",
    "                        name='Net'))\n",
    "\n",
    "    fig1.update_layout(\n",
    "        #title_text=\"Land Gain/Loss & Tree Cover Loss\",\n",
    "        xaxis=dict(\n",
    "            showline=True,\n",
    "            showgrid=True,\n",
    "            showticklabels=True,\n",
    "            linecolor='rgb(204, 204, 204)',\n",
    "            linewidth=2,\n",
    "            ticks='outside',\n",
    "            tickfont=dict(\n",
    "                family='Arial',\n",
    "                size=12,\n",
    "                color='rgb(82, 82, 82)',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ) #height=500, width=1200,\n",
    "    \n",
    "    #chart_outfile = os.path.join(out_dir, 'Chart.png')\n",
    "    #plt.savefig(chart_outfile)\n",
    "    \n",
    "    with out2:\n",
    "        clear_output(wait=True)\n",
    "        display(fig1)\n",
    "        #fig1.show()\n",
    "        #fig1\n",
    "        #print(\"CSV file is now in your download folder.\")\n",
    "        \n",
    "        \n",
    "        #string = areaWithYear  #\"<h5>{}</h5> {}\".format(k,v)\n",
    "        #display(Markdown(string))\n",
    "    \n",
    "        \n",
    "#display(button_csv)\n",
    "button_csv.on_click(csv)\n",
    "\n",
    "#widgets.VBox(children=(out,button_csv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Yearly Seagrass Risk Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b31d7d7aec74cb59ea3a80f34e57060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Button(description='Yearly Seagrass Risk Index', icon='check', layout=Layout(width='3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-15 09:49:08,464 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:10,690 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:11,214 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:11,772 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:12,314 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:12,792 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:13,333 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:13,832 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:14,428 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:15,011 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:15,565 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:16,084 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:16,624 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:17,156 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:17,728 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:18,222 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:18,724 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:19,248 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:19,898 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:20,490 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n",
      "2020-10-15 09:49:20,922 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:20,949 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:20,977 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,005 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,036 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,061 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,089 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,120 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,152 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,178 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,204 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,236 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,261 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,288 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,318 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,347 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,373 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,399 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:21,432 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "grass_out2=widgets.Output()\n",
    "#out.clear_output()\n",
    "\n",
    "grass_vbox2 = widgets.VBox(children=(grass_out2,button_riskindex_year))\n",
    "display(grass_vbox2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib widget\n",
    "import pyproj    \n",
    "import shapely\n",
    "import shapely.ops as ops\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry import Point\n",
    "from functools import partial\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Define Button\n",
    "def convert2(shp2):\n",
    "    '''\n",
    "    When we export the TIF file from GEE, the file that contains all the loss data, the resulting raster will have multiple bands \n",
    "    each representing different annaul loss. Before we calculate the distance between seagrass & possible erosion places we need to \n",
    "    - Export each TIF band to seperate shapefiles\n",
    "    - Select attribute for loss polygons (0=no change 1=loss). We only need 1=loss\n",
    "    - Merge all these polygons. Now we have a single merge.shp file that represents the cumulative (total) erosion areas for all years\n",
    "    - Import this merge file & your seagrass file (must be named Seagrass.shp , case sensetive)\n",
    "    - Project these files\n",
    "    - Find centroid of polygons\n",
    "    - Calculate nearest distance between seagrass centroids to erosion centroids\n",
    "    - Edit dataframe & save risk index shape file\n",
    "    - Map it\n",
    "    '''\n",
    "    ######################## SEAGRASS ########################\n",
    "\n",
    "    #Open seagrass file & project\n",
    "    # Open Seagrass File and Change Projection\n",
    "    seagrass_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Seagrass/Seagrass.shp') \n",
    "    seagrass_poly = geopandas.read_file(seagrass_path)\n",
    "\n",
    "    # Set Projection\n",
    "    seagrass_poly.crs = {'init' :'EPSG:4979'}\n",
    "    #seagrass_poly.to_crs(epsg=32738)\n",
    "\n",
    "    # Set Index\n",
    "    seagrass_poly.index = [x for x in range(0, len(seagrass_poly.values))]\n",
    "    seagrass_poly.index.name = 'id'\n",
    "    #seagrass_poly\n",
    "    \n",
    "    # Centroid\n",
    "    # copy \n",
    "    seagrass_centroid = seagrass_poly.copy()\n",
    "\n",
    "    # change geometry \n",
    "    seagrass_centroid['geometry'] = seagrass_centroid['geometry'].centroid\n",
    "    \n",
    "    ######################## DISTANCE FUNCTION ########################\n",
    "    # Source link: https://automating-gis-processes.github.io/site/notebooks/L3/nearest-neighbor-faster.html\n",
    "    \n",
    "    def get_nearest(src_points, candidates, k_neighbors=1):\n",
    "        #Find nearest neighbors for all source points from a set of candidate points\n",
    "\n",
    "        # Create tree from the candidate points\n",
    "        tree = BallTree(candidates, leaf_size=15, metric='haversine')\n",
    "\n",
    "        # Find closest points and distances\n",
    "        distances, indices = tree.query(src_points, k=k_neighbors)\n",
    "\n",
    "        # Transpose to get distances and indices into arrays\n",
    "        distances = distances.transpose()\n",
    "        indices = indices.transpose()\n",
    "\n",
    "        # Get closest indices and distances (i.e. array at index 0)\n",
    "        # note: for the second closest points, you would take index 1, etc.\n",
    "        closest = indices[0]\n",
    "        closest_dist = distances[0]\n",
    "\n",
    "        # Return indices and distances\n",
    "        return (closest, closest_dist)\n",
    "\n",
    "\n",
    "    def nearest_neighbor(left_gdf, right_gdf, return_dist=False):\n",
    "        \"\"\"\n",
    "        For each point in left_gdf, find closest point in right GeoDataFrame and return them.\n",
    "\n",
    "        NOTICE: Assumes that the input Points are in WGS84 projection (lat/lon).\n",
    "        \"\"\"\n",
    "\n",
    "        left_geom_col = left_gdf.geometry.name\n",
    "        right_geom_col = right_gdf.geometry.name\n",
    "\n",
    "        # Ensure that index in right gdf is formed of sequential numbers\n",
    "        right = right_gdf.copy().reset_index(drop=True)\n",
    "\n",
    "        # Parse coordinates from points and insert them into a numpy array as RADIANS\n",
    "        left_radians = np.array(left_gdf[left_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n",
    "        right_radians = np.array(right[right_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n",
    "\n",
    "        # Find the nearest points\n",
    "        # -----------------------\n",
    "        # closest ==> index in right_gdf that corresponds to the closest point\n",
    "        # dist ==> distance between the nearest neighbors (in meters)\n",
    "\n",
    "        closest, dist = get_nearest(src_points=left_radians, candidates=right_radians)\n",
    "\n",
    "        # Return points from right GeoDataFrame that are closest to points in left GeoDataFrame\n",
    "        closest_points = right.loc[closest]\n",
    "\n",
    "        # Ensure that the index corresponds the one in left_gdf\n",
    "        closest_points = closest_points.reset_index(drop=True)\n",
    "\n",
    "        # Add distance if requested\n",
    "        if return_dist:\n",
    "            # Convert to meters from radians\n",
    "            earth_radius = 6371000  # meters\n",
    "            closest_points['distance'] = dist * earth_radius\n",
    "\n",
    "        return closest_points\n",
    "    \n",
    "    ######################## LOSS TIFF TO SHP ########################\n",
    "\n",
    "    # Open TIF file\n",
    "    raster_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/TIF/Total_Max_Loss.tif') \n",
    "    raster = gdal.Open(raster_path) # There are as many bands as change images \n",
    "    band_count = raster.RasterCount\n",
    "    \n",
    "    # Create Folder To Save Shapefiles\n",
    "    \n",
    "    out_dir = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/Loss_Shapefiles')\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    \n",
    "    # Loop thorugh all the bands in the raster. Each bad represent annual loss amount.\n",
    "    for i in range (band_count+1): #(0, band_count)\n",
    "    \n",
    "        band = raster.GetRasterBand(i)\n",
    "        \n",
    "        # Convert raster (tif) to vector (shp)\n",
    "        # Convert to shapefile\n",
    "        drv = ogr.GetDriverByName('ESRI Shapefile')\n",
    "        outfile_path = os.path.join(out_dir, 'Loss_Polygon{}.shp'.format(i))\n",
    "        outfile = drv.CreateDataSource(outfile_path) \n",
    "        outlayer = outfile.CreateLayer('polygonized raster', srs = None )\n",
    "        newField = ogr.FieldDefn('Value', ogr.OFTReal)\n",
    "        outlayer.CreateField(newField)\n",
    "        \n",
    "        #from contextlib import suppress #This helps ignoring execptions \n",
    "        with suppress(Exception):\n",
    "            \n",
    "            gdal.Polygonize(band, None, outlayer, 0, []) #Received a NULL pointer.\n",
    "            outfile = None\n",
    "    \n",
    "    # Remove the first shapefile (shp, dbf and shx) gdal creates an empty file (why?)\n",
    "    shapefile0 = os.path.join(out_dir, 'Loss_Polygon0.shp')\n",
    "    shapefile0dbf = os.path.join(out_dir, 'Loss_Polygon0.dbf')\n",
    "    shapefile0shx = os.path.join(out_dir, 'Loss_Polygon0.shx')\n",
    "\n",
    "    ## delete only if file exists ##\n",
    "    if os.path.exists(shapefile0):\n",
    "        os.remove(shapefile0)\n",
    "        os.remove(shapefile0dbf)\n",
    "        os.remove(shapefile0shx)\n",
    "    else:\n",
    "        print(\"Sorry, I can not remove %s file.\" % shapefile0)\n",
    "    \n",
    "    \n",
    "    ######################## CENTROID & DISTANCE FOR EACH LOSS FILE ########################\n",
    "    \n",
    "    # Count files\n",
    "    import fnmatch\n",
    "    file_count = len(fnmatch.filter(os.listdir(out_dir), '*.shp'))\n",
    "    \n",
    "    \n",
    "    for i in range (file_count):\n",
    "                    \n",
    "        # Open Merged Loss shapefile as dataframe\n",
    "        # Open as geodataframe\n",
    "        year = df_year[i]\n",
    "        file_num = i+1\n",
    "        loss_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/Loss_Shapefiles/Loss_Polygon{}.shp'.format(file_num)) \n",
    "\n",
    "        # Read as Geopandas\n",
    "        loss_poly = geopandas.read_file(loss_path)\n",
    "\n",
    "        # Set Projection\n",
    "        loss_poly.crs = {'init' :'EPSG:4979'} # Unit in EPSG:3857 & 4979 is in metres WGS 84 Pseudo-Mercator // Unit in EPSG:4326 is degrees WGS 84 World Geodetic System 1984 // Unit in EPSG:32738 is in metres and is for Madagascar\n",
    "\n",
    "    \n",
    "        # Centroid\n",
    "        # Compute Centroid Points for Loss Polygon \n",
    "        # copy GeoDataFrame\n",
    "        loss_centroid = loss_poly.copy()\n",
    "\n",
    "        # change geometry \n",
    "        loss_centroid['geometry'] = loss_centroid['geometry'].centroid\n",
    "\n",
    "    \n",
    "        # Distance \n",
    "\n",
    "        # Calculate\n",
    "        ##---------- Calculate Distance --------- ##\n",
    "        # Find closest erosion point for each seagrass and get also the distance based on haversine distance\n",
    "        # Note: haversine distance which is implemented here is a bit slower than using e.g. 'euclidean' metric\n",
    "        # but useful as we get the distance between points in meters\n",
    "        closest_stops = nearest_neighbor(seagrass_centroid, loss_centroid, return_dist=True)  #seagrass_poly & loss_centroid\n",
    "\n",
    "        # Result\n",
    "        #print(\"closest_stops\", closest_stops)\n",
    "\n",
    "        # Merge result (Distance) with seagrass polygon (not centroid)\n",
    "        # Rename the geometry of closest land loss point so that we can easily identify it\n",
    "        closest_stops = closest_stops.rename(columns={'geometry': 'closest_land_loss'})\n",
    "        #print(\"closest_stops\", closest_stops)\n",
    "\n",
    "        # Merge the datasets by index (for this, it is good to use '.join()' -function)\n",
    "        #seagrass_poly = seagrass_poly.join(closest_stops)\n",
    "        seagrass_poly['Distance'] = closest_stops['distance'].astype(int)\n",
    "        #print(seagrass_poly)\n",
    "\n",
    "        # Calculate Index\n",
    "\n",
    "        # Calculate Height x Distance\n",
    "        seagrass_poly['HxD'] = (seagrass_poly['Avg_Height'] * seagrass_poly['Distance']).astype(int)\n",
    "\n",
    "        # Calculate Linear Scale \n",
    "        seagrass_poly['LinearScale'] = (seagrass_poly['HxD'] - seagrass_poly['HxD'].min())/(seagrass_poly['HxD'].max()-seagrass_poly['HxD'].min())\n",
    "        #print(seagrass_poly)\n",
    "\n",
    "        # Save\n",
    "        out_dir = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/Index')\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        \n",
    "        \n",
    "        seagrass_outfile = os.path.join(out_dir, 'Seagrass_Risk_Index_{}.shp'.format(year) )\n",
    "        seagrass_poly.to_file(seagrass_outfile)\n",
    "        \n",
    "        ######################## MAP & PNG ########################\n",
    "        \n",
    "        ###### Read \n",
    "        #shapefile = emap.shp_to_ee(seagrass_outfile) ## ee.featurecollection #'columns': {'Distance': 'Float', 'HeightCM': 'Integer', 'HxD':\n",
    "\n",
    "        ###### Seagrass Index Visualisation\n",
    "        \n",
    "        # Link: Under Set symbology based on column values. https://github.com/giswqs/geemap/blob/ea409e2f7692909995f080b64945e80f390a3e49/examples/notebooks/select_features.ipynb\n",
    "        #palette = ['B83B5E','F08A5D', 'F8E796' ] # 1orange-purpel 2orange 3honey yellow\n",
    "        #palette = ['F8B195', 'F67280', 'C06C84', '6C5B7B', '355C7D']\n",
    "\n",
    "        #visParams = {'palette': palette,'opacity': 1}\n",
    "\n",
    "        #seagrass = ee.FeatureCollection(shapefile)\n",
    "\n",
    "        #image = ee.Image().float().paint(seagrass, 'LinearScal') \n",
    "        \n",
    "        #Map1.addLayer(image, visParams, 'Seagrass Risk Index_{}'.format(year))\n",
    "        \n",
    "        #legend_keys = ['Risk 1', 'Risk 2', 'Risk 3', 'Risk 4', 'Risk 5'] \n",
    "        #legend_colors = palette\n",
    "        #Map1.add_legend(legend_keys=legend_keys, legend_colors=legend_colors, position='bottomright')\n",
    "        \n",
    "        ###### Seagrass Index Plot & PNG\n",
    "        \n",
    "        # Configure PLOT\n",
    "        # Link: https://geopandas.org/gallery/plotting_with_geoplot.html?highlight=classify\n",
    "\n",
    "        # Chose What to categorise, divide to 5 equal intervals\n",
    "        #categories = seagrass_poly['LinearScale']\n",
    "        #scheme = mapclassify.Quantiles(categories, k=5) # This is is quantile classification\n",
    "        #choropleth = geoplot.choropleth(seagrass_poly, hue=categories, scheme=scheme, legend=True,cmap='Greens', figsize=(12, 12))\n",
    "        #fig = plt.gcf()\n",
    "        \n",
    "        #Save\n",
    "        #index_image_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/Index/Seagrass_Risk_Index{}.png'.format(year))\n",
    "        #fig.savefig(index_image_path, dpi=100)\n",
    "    \n",
    "    ######################## BAR CHART ########################\n",
    "    ###### Create Dataframe & Plot Bar Chart\n",
    "    out_dir = os.path.join(os.path.expanduser('~'), '/Users/nat/Downloads/Seagrass_Risk_Index/Output/Index/')\n",
    "    file_count = len(fnmatch.filter(os.listdir(out_dir), '*.shp'))\n",
    "    #print(file_count)\n",
    "\n",
    "    scale_list = []\n",
    "    species = []\n",
    "    for i in range (file_count):\n",
    "        year = df_year[i]\n",
    "\n",
    "        shp_dir = os.path.join(os.path.expanduser('~'), '/Users/nat/Downloads/Seagrass_Risk_Index/Output/Index/Seagrass_Risk_Index_{}.shp'.format(year))\n",
    "\n",
    "        gdf = geopandas.read_file(shp_dir)\n",
    "        #print(gdf)\n",
    "\n",
    "        scale_list.append(gdf['LinearScal'])\n",
    "        species.append(gdf['Species'])\n",
    "\n",
    "\n",
    "    # Create the pandas DataFrame \n",
    "    df = pd.DataFrame(scale_list) \n",
    "    df = df.transpose() \n",
    "    #print(df)\n",
    "\n",
    "\n",
    "    df.columns = df_year\n",
    "    #print(df)\n",
    "\n",
    "    df['Species'] = species[0]\n",
    "    #print(df)\n",
    "    \n",
    "    # Get df shape & Select Years (Exclude Species)\n",
    "    shape = df.shape  # (174, 4)\n",
    "    column_num = shape[1] # 4\n",
    "    slice_num = column_num - 1\n",
    "    year_columns = df.iloc[:,:slice_num]\n",
    "    \n",
    "    # Pivot with single index\n",
    "    table = pd.pivot_table(data=df,index=['Species'])\n",
    "    df2 = pd.DataFrame(table)\n",
    "\n",
    "    # Convert Pivot table to dataframe\n",
    "    flattened = pd.DataFrame(df2.to_records())\n",
    "\n",
    "    # Set index\n",
    "    flattened.reset_index(drop=True, inplace=True)\n",
    "    flattened.set_index('Species', inplace=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_dir = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/CSV')\n",
    "    df_outfile = os.path.join(csv_dir, 'Seagrass_Risk_Index.csv')\n",
    "    df.to_csv(df_outfile, index = True)\n",
    "\n",
    "   \n",
    "    with grass_out2: \n",
    "        clear_output(wait=True)\n",
    "\n",
    "        #index = flattened['Species']\n",
    "        #fig = plt.figure()\n",
    "        ax = flattened.plot.bar(stacked=True, cmap=cm.Spectral) # \n",
    "        plt.xticks(rotation=25)\n",
    "        ax.set_facecolor('xkcd:black')\n",
    "        #fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "        #fig.show()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        \n",
    "button_riskindex_year.on_click(convert2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cumulative Seagrass Risk Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d786a611c3ea4eaea5a98a1cee5d0357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Button(description='Cumulative Seagrass Risk Index', icon='check', layout=Layout(widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-15 09:49:54,852 [12494] ERROR    fiona._env:207: [JupyterRequire] Unable to open EPSG support file gcs.csv.  Try setting the GDAL_DATA environment variable to point to the directory containing EPSG csv files.\n",
      "2020-10-15 09:49:55,935 [12494] WARNING  fiona._env:157: [JupyterRequire] Normalized/laundered field name: 'LinearScale' to 'LinearScal'\n"
     ]
    }
   ],
   "source": [
    "grass_out=widgets.Output()\n",
    "#out.clear_output()\n",
    "grass_vbox = widgets.VBox(children=(grass_out,button_riskindex))\n",
    "display(grass_vbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "import pyproj    \n",
    "import shapely\n",
    "import shapely.ops as ops\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry import Point\n",
    "from functools import partial\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Define Button\n",
    "def convert(shp):\n",
    "    '''\n",
    "    When we export the TIF file from GEE, the file that contains all the loss data, the resulting raster will have multiple bands \n",
    "    each representing different annaul loss. Before we calculate the distance between seagrass & possible erosion places we need to \n",
    "    - Export each TIF band to seperate shapefiles\n",
    "    - Select attribute for loss polygons (0=no change 1=loss). We only need 1=loss\n",
    "    - Merge all these polygons. Now we have a single merge.shp file that represents the cumulative (total) erosion areas for all years\n",
    "    - Import this merge file & your seagrass file (must be named Seagrass.shp , case sensetive)\n",
    "    - Project these files\n",
    "    - Find centroid of polygons\n",
    "    - Calculate nearest distance between seagrass centroids to erosion centroids\n",
    "    - Edit dataframe & save risk index shape file\n",
    "    - Map it\n",
    "    '''\n",
    "    \n",
    "    ######################## LOSS TIFF ########################\n",
    "\n",
    "    ######################## 1: Open TIF file\n",
    "    raster_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/TIF/Total_Max_Loss.tif') \n",
    "    raster = gdal.Open(raster_path) # There are as many bands as change images \n",
    "    band_count = raster.RasterCount\n",
    "    \n",
    "    ######################## 2: Create Folder To Save Shapefiles\n",
    "    \n",
    "    out_dir = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/Loss_Shapefiles')\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    \n",
    "    ######################## 3: Loop thorugh all the bands in the raster. Each bad represent annual loss amount.\n",
    "    for i in range (band_count+1): #(0, band_count)\n",
    "    \n",
    "        band = raster.GetRasterBand(i)\n",
    "        #print(type(band)) \n",
    "        \n",
    "        ######################## 4: Convert raster (tif) to vector (shp)\n",
    "        # Convert to shapefile\n",
    "        drv = ogr.GetDriverByName('ESRI Shapefile')\n",
    "        outfile_path = os.path.join(out_dir, 'Loss_Polygon{}.shp'.format(i))\n",
    "        outfile = drv.CreateDataSource(outfile_path) \n",
    "        outlayer = outfile.CreateLayer('polygonized raster', srs = None )\n",
    "        newField = ogr.FieldDefn('Value', ogr.OFTReal)\n",
    "        outlayer.CreateField(newField)\n",
    "        \n",
    "        #from contextlib import suppress #This helps ignoring execptions \n",
    "        with suppress(Exception):\n",
    "        #with out:\n",
    "            \n",
    "            gdal.Polygonize(band, None, outlayer, 0, []) #Received a NULL pointer.\n",
    "            outfile = None\n",
    "    \n",
    "    ######################## 5: Remove the first shapefile (shp, dbf and shx) gdal creates an empty file (why?)\n",
    "    shapefile0 = os.path.join(out_dir, 'Loss_Polygon0.shp')\n",
    "    shapefile0dbf = os.path.join(out_dir, 'Loss_Polygon0.dbf')\n",
    "    shapefile0shx = os.path.join(out_dir, 'Loss_Polygon0.shx')\n",
    "    \n",
    "    ## get input \n",
    "    #filename=raw_input(\"Type file name to remove: \")\n",
    "\n",
    "    ## delete only if file exists ##\n",
    "    if os.path.exists(shapefile0):\n",
    "        os.remove(shapefile0)\n",
    "        os.remove(shapefile0dbf)\n",
    "        os.remove(shapefile0shx)\n",
    "    else:\n",
    "        print(\"Sorry, I can not remove %s file.\" % shapefile0)\n",
    "    \n",
    "      \n",
    "    ######################## SHAPEFILES ########################\n",
    "    \n",
    "    ######################## 7: Select by Attribute & Merge Shapefiles\n",
    "    \n",
    "\n",
    "    # Link http://pcjericks.github.io/py-gdalogr-cookbook/vector_layers.html#merge-ogr-layers\n",
    "    #directory = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Loss_Shapefiles')\n",
    "    outputMergefn = os.path.join(out_dir, 'Merge.shp') # Output directory and file name\n",
    "    fileStartsWith = 'Loss_Polygon'\n",
    "    fileEndsWith = '.shp'\n",
    "    driverName = 'ESRI Shapefile'\n",
    "    geometryType = ogr.wkbPolygon\n",
    "\n",
    "    out_driver = ogr.GetDriverByName( driverName )\n",
    "    if os.path.exists(outputMergefn):\n",
    "        out_driver.DeleteDataSource(outputMergefn)\n",
    "    out_ds = out_driver.CreateDataSource(outputMergefn)\n",
    "    out_layer = out_ds.CreateLayer(outputMergefn, geom_type=geometryType)\n",
    "\n",
    "    fileList = os.listdir(out_dir) # The file list is specified above\n",
    "\n",
    "    for file in fileList:\n",
    "        #if file.startswith(fileStartsWith) and file.endswith(fileEndsWith):\n",
    "            #print (file)\n",
    "        if file.endswith(\".shp\"):\n",
    "            shp_path = os.path.join(out_dir, file)\n",
    "            ds = ogr.Open(shp_path) #ds = datasource\n",
    "            #ds = ogr.Open(out_dir+file)\n",
    "            lyr = ds.GetLayer() # lyr = layer\n",
    "            lyr.SetAttributeFilter(\"Value = '1'\") #Select the polygons where value=1 (loss)\n",
    "            for feat in lyr:\n",
    "                out_feat = ogr.Feature(out_layer.GetLayerDefn())\n",
    "                out_feat.SetGeometry(feat.GetGeometryRef().Clone())\n",
    "                out_layer.CreateFeature(out_feat)\n",
    "                out_feat = None\n",
    "                out_layer.SyncToDisk()\n",
    "    \n",
    "    ######################## SEAGRASS & LOSS ########################\n",
    "                \n",
    "    ######################## 8: Open Merged Loss shapefile as dataframe\n",
    "    # Open as geodataframe\n",
    "    loss_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/Loss_Shapefiles/Merge.shp') \n",
    "    \n",
    "    # Read as Geopandas\n",
    "    loss_poly = geopandas.read_file(loss_path)\n",
    "\n",
    "    # Set Projection\n",
    "    loss_poly.crs = {'init' :'EPSG:4979'} # Unit in EPSG:3857 & 4979 is in metres WGS 84 Pseudo-Mercator // Unit in EPSG:4326 is degrees WGS 84 World Geodetic System 1984 // Unit in EPSG:32738 is in metres and is for Madagascar\n",
    "    #print(loss_poly.crs)\n",
    "    \n",
    "#     # Calculate area for merged erosion\n",
    "#     geom = loss_poly['geometry'][0]\n",
    "#     geom_area = ops.transform(\n",
    "#         partial(\n",
    "#             pyproj.transform,\n",
    "#             pyproj.Proj(init='EPSG:4979'),\n",
    "#             pyproj.Proj(\n",
    "#                 proj='aea', #aea 'utm'\n",
    "#                 lat_1=geom.bounds[1],\n",
    "#                 lat_2=geom.bounds[3])),\n",
    "#         geom)\n",
    "\n",
    "\n",
    "#     km2 = geom_area.area / 1000000\n",
    "    \n",
    "#     if km2 > 30:\n",
    "#         # CLIP\n",
    "#         coord = coordinates[0] # centroid for AOI\n",
    "#         ## Set X & Y\n",
    "#         x = df_xy.iloc[0]['XY']\n",
    "#         y = df_xy.iloc[1]['XY']\n",
    "#         p1 = Point((x,y))\n",
    "\n",
    "#         ## Read as dataframe\n",
    "#         df = pd.DataFrame({'X':[x], 'Y':[y]}) # x and y has to be in [] because data values has be list or dict values.\n",
    "#         gdf = geopandas.GeoDataFrame(df, geometry = [p1])\n",
    "        \n",
    "#         # Buffer the points by 0.01 units = approx 30 km2\n",
    "#         buffer = gdf.buffer(0.025)\n",
    "#         # Apply an envelope around circular buffers\n",
    "#         envelope = buffer.envelope  \n",
    "        \n",
    "#         # Envelope geodataframe\n",
    "#         envelope_gdf = geopandas.GeoDataFrame(gpd.GeoSeries(envelope))\n",
    "\n",
    "#         # Set geometry for resulting geodataframe\n",
    "#         envelope_gdf = envelope_gdf.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "#         envelope_gdf.crs = {'init' :'EPSG:4979'}\n",
    "        \n",
    "#         # Clip\n",
    "#         clip = geopandas.clip(loss_poly, envelope_gdf)\n",
    "        \n",
    "#         # Save\n",
    "#         out_dir = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Loss_Shapefiles')\n",
    "#         clip_outfile = os.path.join(out_dir, 'Loss_clipped.shp')\n",
    "#         clip.to_file(clip_outfile)\n",
    "        \n",
    "#         # Open\n",
    "#         clipped_poly = geopandas.read_file(clip_outfile)\n",
    "\n",
    "#         # Set Projection\n",
    "#         clipped_poly.crs = {'init' :'EPSG:4979'} # Unit in EPSG:3857 & 4979 is in metres WGS 84 Pseudo-Mercator // Unit in EPSG:4326 is degrees WGS 84 World Geodetic System 1984 // Unit in EPSG:32738 is in metres and is for Madagascar\n",
    "        \n",
    "        \n",
    "#         # Define GEE shapefile to map\n",
    "#         loss_shapefile = emap.shp_to_ee(clipped_poly)\n",
    "    \n",
    "#     else:\n",
    "#         # Define GEE shapefile from original file (without clip)\n",
    "          #loss_shapefile = emap.shp_to_ee(loss_path) # FeatureCollection\n",
    "    \n",
    "#     ###############  CLIP ############### \n",
    "#     coord = coordinates[0] # centroid for AOI\n",
    "#     df_xy = pd.DataFrame({'XY':coord})\n",
    "#     ## Set X & Y\n",
    "#     x = df_xy.iloc[0]['XY']\n",
    "#     y = df_xy.iloc[1]['XY']\n",
    "#     p1 = Point((x,y))\n",
    "\n",
    "#     ## Read as dataframe\n",
    "#     df = pd.DataFrame({'X':[x], 'Y':[y]}) # x and y has to be in [] because data values has be list or dict values.\n",
    "#     gdf = geopandas.GeoDataFrame(df, geometry = [p1])\n",
    "    \n",
    "#     # Buffer the points by 2 units\n",
    "#     buffer = gdf.buffer(0.025)\n",
    "\n",
    "#     # Apply an envelope around circular buffers\n",
    "#     envelope = buffer.envelope  \n",
    "    \n",
    "#     data = envelope\n",
    "#     df_env = pd.DataFrame(data) # POLYGON ((48.45515 -13.55007, 48.50515 -13.55007, 48.50515\n",
    "#     # Rename Column\n",
    "#     df_env.rename(columns={0: 'Geom'}, inplace = True) # pandas.core.series\n",
    "#     # Edit Column\n",
    "#     df_env['Geom'] = df_env['Geom'].astype(str) \n",
    "#     df_env['Geom'] = df_env['Geom'].str.replace('POLYGON','')\n",
    "#     df_env['Geom'] = df_env['Geom'].str.replace('(','')\n",
    "#     df_env['Geom'] = df_env['Geom'].str.replace(')','')\n",
    "#     df_env[['P1','P2', 'P3', 'P4', 'P5']] = df_env.Geom.str.split(\",\",expand=True)\n",
    "#     #df_env[['P1', 'null1', 'P1x', 'P1y', 'P2', 'null2', 'P2x', 'P2y', 'P3', 'null3', 'P3x', 'P3y', 'P4', 'null4', 'P4x', 'P4y', 'P5', 'null5', 'P5x', 'P5y']] = df_env.Geom.str.split(\" \",expand=True)\n",
    "\n",
    "\n",
    "#     # Delete Geom Column & Keep P5, its a repetation of P1 but necessary for Polygon\n",
    "#     del df_env['Geom']\n",
    "\n",
    "#     P1df = pd.DataFrame(df_env['P1']) \n",
    "#     P1df[['a','P1x', 'P1y']] = P1df.P1.str.split(\" \",expand=True)\n",
    "#     del P1df['a']\n",
    "#     #print(P1df)\n",
    "\n",
    "#     P2df = pd.DataFrame(df_env['P2']) \n",
    "#     P2df[['a','P2x', 'P2y']] = P2df.P2.str.split(\" \",expand=True)\n",
    "#     del P2df['a']\n",
    "#     #print(P2df)\n",
    "\n",
    "#     P3df = pd.DataFrame(df_env['P3']) \n",
    "#     P3df[['a','P3x', 'P3y']] = P3df.P3.str.split(\" \",expand=True)\n",
    "#     del P3df['a']\n",
    "#     #print(P3df)\n",
    "\n",
    "#     P4df = pd.DataFrame(df_env['P4']) \n",
    "#     P4df[['a','P4x', 'P4y']] = P4df.P4.str.split(\" \",expand=True)\n",
    "#     del P4df['a']\n",
    "#     #print(P4df)\n",
    "\n",
    "#     P5df = pd.DataFrame(df_env['P5']) \n",
    "#     P5df[['a','P5x', 'P5y']] = P5df.P5.str.split(\" \",expand=True)\n",
    "#     del P5df['a']\n",
    "    \n",
    "#     # Convert to float and then to a list and then to EE Number\n",
    "#     x1 = P1df['P1x'].astype(float).round(10).tolist()\n",
    "#     y1 = P1df['P1y'].astype(float).round(10).tolist()\n",
    "\n",
    "\n",
    "#     x2 = P2df['P2x'].astype(float).round(10).tolist()\n",
    "#     y2 = P2df['P2y'].astype(float).round(8).tolist()\n",
    "\n",
    "#     x3 = P3df['P3x'].astype(float).round(10).tolist()\n",
    "#     y3 = P3df['P3y'].astype(float).round(10).tolist()\n",
    "\n",
    "#     x4 = P4df['P4x'].astype(float).round(10).tolist()\n",
    "#     y4 = P4df['P4y'].astype(float).round(10).tolist()\n",
    "\n",
    "#     x5 = P5df['P5x'].astype(float).round(10).tolist()\n",
    "#     y5 = P5df['P5y'].astype(float).round(10).tolist()\n",
    "    \n",
    "#     ee_x1 = ee.Number(x1[0])\n",
    "#     ee_y1 = ee.Number(y1[0])\n",
    "\n",
    "#     ee_x2 = ee.Number(x2[0])\n",
    "#     ee_y2 = ee.Number(y2[0])\n",
    "\n",
    "#     ee_x3 = ee.Number(x3[0])\n",
    "#     ee_y3 = ee.Number(y3[0])\n",
    "\n",
    "#     ee_x4 = ee.Number(x4[0])\n",
    "#     ee_y4 = ee.Number(y4[0])\n",
    "\n",
    "#     ee_x5 = ee.Number(x5[0])\n",
    "#     ee_y5 = ee.Number(y5[0])\n",
    "    \n",
    "#     bbox = ee.Geometry.Polygon([[[ee_x1, ee_y1], [ee_x2, ee_y2], [ee_x3, ee_y3], [ee_x4, ee_y4], [ee_x5, ee_y5]]])\n",
    "\n",
    "#     # Filter merged loss erosion files\n",
    "#     loss_filtered = loss_shapefile.filterBounds(bbox)\n",
    "    \n",
    "    ######################## 9: Centroid\n",
    "    # Compute Centroid Points for Loss Polygon \n",
    "    # copy GeoDataFrame\n",
    "    loss_centroid = loss_poly.copy()\n",
    "\n",
    "    # change geometry \n",
    "    loss_centroid['geometry'] = loss_centroid['geometry'].centroid\n",
    "\n",
    "    #loss_centroid.head()\n",
    "    #loss_centroid.plot()\n",
    "    \n",
    "    ######################## Seagrass ########################\n",
    "\n",
    "    ######################## 10: Open seagrass file & project\n",
    "    # Open Seagrass File and Change Projection\n",
    "    seagrass_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Seagrass/Seagrass.shp') \n",
    "    seagrass_poly = geopandas.read_file(seagrass_path)\n",
    "\n",
    "    # Set Projection\n",
    "    seagrass_poly.crs = {'init' :'EPSG:4979'}\n",
    "    #seagrass_poly.to_crs(epsg=32738)\n",
    "\n",
    "    # Set Index\n",
    "    seagrass_poly.index = [x for x in range(0, len(seagrass_poly.values))]\n",
    "    seagrass_poly.index.name = 'id'\n",
    "    #seagrass_poly\n",
    "    \n",
    "    ######################## 11: Centroid\n",
    "    # Compute Centroid Points for Seagrass Polygon \n",
    "    # copy \n",
    "    seagrass_centroid = seagrass_poly.copy()\n",
    "\n",
    "    # change geometry \n",
    "    seagrass_centroid['geometry'] = seagrass_centroid['geometry'].centroid\n",
    "    \n",
    "    ######################## Distance ########################\n",
    "\n",
    "    ######################## 12: Distance Functions\n",
    "    # Source link: https://automating-gis-processes.github.io/site/notebooks/L3/nearest-neighbor-faster.html\n",
    "    \n",
    "    def get_nearest(src_points, candidates, k_neighbors=1):\n",
    "        #Find nearest neighbors for all source points from a set of candidate points\n",
    "\n",
    "        # Create tree from the candidate points\n",
    "        tree = BallTree(candidates, leaf_size=15, metric='haversine')\n",
    "\n",
    "        # Find closest points and distances\n",
    "        distances, indices = tree.query(src_points, k=k_neighbors)\n",
    "\n",
    "        # Transpose to get distances and indices into arrays\n",
    "        distances = distances.transpose()\n",
    "        indices = indices.transpose()\n",
    "\n",
    "        # Get closest indices and distances (i.e. array at index 0)\n",
    "        # note: for the second closest points, you would take index 1, etc.\n",
    "        closest = indices[0]\n",
    "        closest_dist = distances[0]\n",
    "\n",
    "        # Return indices and distances\n",
    "        return (closest, closest_dist)\n",
    "\n",
    "\n",
    "\n",
    "    def nearest_neighbor(left_gdf, right_gdf, return_dist=False):\n",
    "        \"\"\"\n",
    "        For each point in left_gdf, find closest point in right GeoDataFrame and return them.\n",
    "\n",
    "        NOTICE: Assumes that the input Points are in WGS84 projection (lat/lon).\n",
    "        \"\"\"\n",
    "\n",
    "        left_geom_col = left_gdf.geometry.name\n",
    "        right_geom_col = right_gdf.geometry.name\n",
    "\n",
    "        # Ensure that index in right gdf is formed of sequential numbers\n",
    "        right = right_gdf.copy().reset_index(drop=True)\n",
    "\n",
    "        # Parse coordinates from points and insert them into a numpy array as RADIANS\n",
    "        left_radians = np.array(left_gdf[left_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n",
    "        right_radians = np.array(right[right_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n",
    "\n",
    "        # Find the nearest points\n",
    "        # -----------------------\n",
    "        # closest ==> index in right_gdf that corresponds to the closest point\n",
    "        # dist ==> distance between the nearest neighbors (in meters)\n",
    "\n",
    "        closest, dist = get_nearest(src_points=left_radians, candidates=right_radians)\n",
    "\n",
    "        # Return points from right GeoDataFrame that are closest to points in left GeoDataFrame\n",
    "        closest_points = right.loc[closest]\n",
    "\n",
    "        # Ensure that the index corresponds the one in left_gdf\n",
    "        closest_points = closest_points.reset_index(drop=True)\n",
    "\n",
    "        # Add distance if requested\n",
    "        if return_dist:\n",
    "            # Convert to meters from radians\n",
    "            earth_radius = 6371000  # meters\n",
    "            closest_points['distance'] = dist * earth_radius\n",
    "\n",
    "        return closest_points\n",
    "    \n",
    "    ######################## 13: Calculate\n",
    "    ##---------- Calculate Distance --------- ##\n",
    "    # Find closest erosion point for each seagrass and get also the distance based on haversine distance\n",
    "    # Note: haversine distance which is implemented here is a bit slower than using e.g. 'euclidean' metric\n",
    "    # but useful as we get the distance between points in meters\n",
    "    closest_stops = nearest_neighbor(seagrass_centroid, loss_centroid, return_dist=True)  #seagrass_poly & loss_centroid\n",
    "\n",
    "    # Result\n",
    "    #closest_stops\n",
    "    \n",
    "    ######################## 3: Merge result (Distance) with seagrass polygon (not centroid)\n",
    "    # Rename the geometry of closest land loss point so that we can easily identify it\n",
    "    closest_stops = closest_stops.rename(columns={'geometry': 'closest_land_loss'})\n",
    "\n",
    "    # Merge the datasets by index (for this, it is good to use '.join()' -function)\n",
    "    #seagrass_poly = seagrass_poly.join(closest_stops)\n",
    "    seagrass_poly['Distance'] = closest_stops['distance'].astype(int)\n",
    "    \n",
    "    ######################## 14: Calculate Index\n",
    "\n",
    "    # Calculate Height x Distance\n",
    "    seagrass_poly['HxD'] = (seagrass_poly['Avg_Height'] * seagrass_poly['Distance']).astype(int)\n",
    "\n",
    "    # Calculate Linear Scale \n",
    "    seagrass_poly['LinearScale'] = (seagrass_poly['HxD'] - seagrass_poly['HxD'].min())/(seagrass_poly['HxD'].max()-seagrass_poly['HxD'].min())\n",
    "\n",
    "    # Save\n",
    "    out_dir = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/Index')\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "        \n",
    "    seagrass_outfile = os.path.join(out_dir, 'Seagrass_Risk_Index.shp')\n",
    "    seagrass_poly.to_file(seagrass_outfile)\n",
    "    \n",
    "    # Read\n",
    "    #index_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Seagrass_Risk_Index.shp') \n",
    "    shapefile = emap.shp_to_ee(seagrass_outfile) ## ee.featurecollection #'columns': {'Distance': 'Float', 'HeightCM': 'Integer', 'HxD':\n",
    "    \n",
    "    # Seagrass Visualisation\n",
    "    # Link: Under Set symbology based on column values. https://github.com/giswqs/geemap/blob/ea409e2f7692909995f080b64945e80f390a3e49/examples/notebooks/select_features.ipynb\n",
    "    palette = ['B83B5E','F08A5D', 'F8E796' ] # 1orange-purpel 2orange 3honey yellow\n",
    "    palette = ['F8B195', 'F67280', 'C06C84', '6C5B7B', '355C7D']\n",
    "    \n",
    "    visParams = {\n",
    "      'palette': palette,\n",
    "      'opacity': 1,\n",
    "    }\n",
    "\n",
    "    seagrass = ee.FeatureCollection(shapefile)\n",
    "\n",
    "    image = ee.Image().float().paint(seagrass, 'LinearScal') \n",
    "    \n",
    "    #legend_keys = ['Risk 1', 'Risk 2', 'Risk 3'] #'Risk 5', 'Risk 4', \n",
    "    legend_keys = ['Risk 1', 'Risk 2', 'Risk 3', 'Risk 4', 'Risk 5'] \n",
    "    \n",
    "    legend_colors = palette\n",
    "    \n",
    "    # Loss Shapefile Visulaisation\n",
    "    visParams2 = {\n",
    "      'color': '00dfff',\n",
    "      'opacity': 0.8,\n",
    "    }\n",
    "    \n",
    "    # Configure PLOT\n",
    "    # Link: https://geopandas.org/gallery/plotting_with_geoplot.html?highlight=classify\n",
    "    \n",
    "    # Chose What to categorise, divide to 5 equal intervals\n",
    "    categories = seagrass_poly['LinearScale']\n",
    "    scheme = mapclassify.Quantiles(categories, k=5) # This is is quantile classification\n",
    "    #scheme = mapclassify.EqualInterval(categories, k=3)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Print plot graph\n",
    "    with grass_out:\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        #Map1.add_basemap('CartoDB.DarkMatter') #SATELLITE HYBRID CartoDB.DarkMatter Esri Satellite\n",
    "        Map1.addLayer(image, visParams, 'Seagrass Risk Index')\n",
    "        Map1.add_legend(legend_keys=legend_keys, legend_colors=legend_colors, position='bottomright')\n",
    "        #Map1.addLayer(loss_shapefile, visParams2, 'Total Loss') \n",
    "        #Map1.addLayer(loss_filtered, visParams2, 'Total Loss Clipped') \n",
    "        \n",
    "        \n",
    "        choropleth = geoplot.choropleth(seagrass_poly, hue=categories, scheme=scheme, legend=True,cmap='Greens', figsize=(12, 12))\n",
    "        display(choropleth)\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "        \n",
    "        #fig = plt.gcf()\n",
    "        \n",
    "        #Save\n",
    "        index_image_path = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/Index/Seagrass_Risk_Index.png')\n",
    "        fig.savefig(index_image_path, dpi=100)\n",
    "        #plt.savefig(index_image_path, bbox_inches='tight', pad_inches=0.1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Image(filename=index_image_path)\n",
    "        \n",
    "        #img = mpimg.imread(index_image_path)\n",
    "        #imgplot = plt.imshow(img)\n",
    "        #plt.show()\n",
    "\n",
    "        # Plot 2: With base map\n",
    "        #ax = geoplot.webmap(seagrass_poly, projection=gcrs.WebMercator())\n",
    "        #geoplot.choropleth(seagrass_poly, hue=categories, scheme=scheme, legend=True,cmap='Greens', figsize=(12, 12), ax=ax)\n",
    "\n",
    "#linking button and function together using a button's method\n",
    "button_riskindex.on_click(convert)\n",
    "# displaying button and its output together\n",
    "#widgets.VBox([button_riskindex,out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c7ec375d59418db5cdb641123b18ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Button(description='Total Images Per Year', icon='check', layout=Layout(width='300px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_canada' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-47cf4f6d01fe>\u001b[0m in \u001b[0;36mcount\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Add traces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mfig1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_canada\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#fig1.add_trace(go.Scatter(x=df['Year'], y=df['Count'],mode='lines+markers', name='Count'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_canada' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_canada' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-8a32f34b6144>\u001b[0m in \u001b[0;36mcount\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Add traces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfig1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_canada\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m#fig1.add_trace(go.Scatter(x=df['Year'], y=df['Count'],mode='lines+markers', name='Count'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_canada' is not defined"
     ]
    }
   ],
   "source": [
    "out3=widgets.Output()\n",
    "#out.clear_output()\n",
    "\n",
    "vbox3 = widgets.VBox(children=(out3,button_imagecount))\n",
    "display(vbox3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def count(c):\n",
    "    \n",
    "    count = pd.DataFrame({'Count':image_count, 'Year':all_years })  #df = pd.DataFrame({'Year':df_year})\n",
    "\n",
    "\n",
    "    ######################## 3: Save\n",
    "\n",
    "    # Save to CSV\n",
    "    out_dir = os.path.join(os.path.expanduser('~'), 'Downloads/Seagrass_Risk_Index/Output/CSV')\n",
    "    count_outfile = os.path.join(out_dir, 'Total_Images_Per_Year.csv')\n",
    "    count.to_csv(count_outfile, index = True)\n",
    "    \n",
    "    # Open CSV\n",
    "    df = pd.read_csv(count_outfile)\n",
    "    df.set_index('Year')\n",
    "\n",
    "    fig1 = go.Figure()\n",
    "\n",
    "    # Add traces\n",
    "    fig1 = px.bar(df, x=df['Year'], y=df['Count'])\n",
    "    #fig1.add_trace(go.Scatter(x=df['Year'], y=df['Count'],mode='lines+markers', name='Count'))\n",
    "\n",
    "    fig1.update_layout(\n",
    "        #title_text=\"Land Gain/Loss & Tree Cover Loss\",\n",
    "        xaxis=dict(\n",
    "            showline=True,\n",
    "            showgrid=True,\n",
    "            showticklabels=True,\n",
    "            linecolor='rgb(204, 204, 204)',\n",
    "            linewidth=2,\n",
    "            ticks='outside',\n",
    "            tickfont=dict(\n",
    "                family='Arial',\n",
    "                size=12,\n",
    "                color='rgb(82, 82, 82)',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ) #height=500, width=1200,\n",
    "    \n",
    "    #chart_outfile = os.path.join(out_dir, 'Chart.png')\n",
    "    #plt.savefig(chart_outfile)\n",
    "    \n",
    "    with out3:\n",
    "        clear_output(wait=True)\n",
    "        display(fig1)\n",
    "        \n",
    "        \n",
    "button_imagecount.on_click(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>28</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>26</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>22</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>15</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Count  Year\n",
       "0       0  1990\n",
       "1       0  1991\n",
       "2       0  1992\n",
       "3       0  1993\n",
       "4       9  1994\n",
       "5       6  1995\n",
       "6       6  1996\n",
       "7       4  1997\n",
       "8       6  1998\n",
       "9       5  1999\n",
       "10     13  2000\n",
       "11      6  2001\n",
       "12      5  2002\n",
       "13      8  2003\n",
       "14     12  2004\n",
       "15     11  2005\n",
       "16     15  2006\n",
       "17      9  2007\n",
       "18     11  2008\n",
       "19     11  2009\n",
       "20     10  2010\n",
       "21     10  2011\n",
       "22      8  2012\n",
       "23     28  2013\n",
       "24     23  2014\n",
       "25     26  2015\n",
       "26     28  2016\n",
       "27     31  2017\n",
       "28     26  2018\n",
       "29     22  2019\n",
       "30     15  2020"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = pd.DataFrame({'Count':image_count, 'Year':all_years })  #df = pd.DataFrame({'Year':df_year})\n",
    "count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataframe Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# import fnmatch\n",
    "# import matplotlib.cm as cm\n",
    "\n",
    "# out_dir = os.path.join(os.path.expanduser('~'), '/Users/nat/Downloads/Seagrass_Risk_Index/Output/Index/')\n",
    "# file_count = len(fnmatch.filter(os.listdir(out_dir), '*.shp'))\n",
    "\n",
    "# scale_list = []\n",
    "# species = []\n",
    "# for i in range (file_count):\n",
    "#     year = df_year[i]\n",
    "#     shp_dir = os.path.join(os.path.expanduser('~'), '/Users/nat/Downloads/Seagrass_Risk_Index/Output/Index/Seagrass_Risk_Index_{}.shp'.format(year))\n",
    "   \n",
    "#     gdf = geopandas.read_file(shp_dir)\n",
    "    \n",
    "#     scale_list.append(gdf['LinearScal'])\n",
    "#     species.append(gdf['Species'])\n",
    "    \n",
    "\n",
    "# # Create the pandas DataFrame \n",
    "# df = pd.DataFrame(scale_list) \n",
    "# df = df.transpose() \n",
    "# df.columns = df_year\n",
    "# df['Species'] = species[0]\n",
    "\n",
    "# # Getting shape of the df\n",
    "# shape = df.shape  # (174, 4)\n",
    "# column_num = shape[1] # 4\n",
    "# slice_num = column_num - 1\n",
    "# year_columns = df.iloc[:,:slice_num]\n",
    "\n",
    "\n",
    "# # Pivot with single index\n",
    "# table = pd.pivot_table(data=df,index=['Species'])\n",
    "# df2 = pd.DataFrame(table)\n",
    "\n",
    "# # list columns \n",
    "# #list(df2.columns)\n",
    "\n",
    "# # Select the first column\n",
    "# #df2.iloc[:,:0]\n",
    "\n",
    "# # Pivot to dataframe\n",
    "# flattened = pd.DataFrame(df2.to_records())\n",
    "\n",
    "# # Set index\n",
    "# flattened.set_index('Species')\n",
    "# flattened.reset_index(drop=True, inplace=True)\n",
    "# flattened.set_index('Species', inplace=True)\n",
    "\n",
    "# ax = flattened.plot.bar(stacked=True, cmap=cm.Spectral)\n",
    "# plt.xticks(rotation=45)\n",
    "# ax.set_facecolor('xkcd:black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "396px",
    "left": "10px",
    "top": "923px",
    "width": "273.844px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
